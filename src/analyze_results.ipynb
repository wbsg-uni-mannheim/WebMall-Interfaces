{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def generate_csv_metrics(enhanced_summary):\n",
    "    \"\"\"Generate CSV file with task metrics for easy analysis.\"\"\"\n",
    "    if not enhanced_summary.get(\"results\"):\n",
    "        print(\"No results to export to CSV\")\n",
    "        return\n",
    "\n",
    "    current_timestamp = enhanced_summary[\"benchmark_metadata\"][\"timestamp\"]\n",
    "    csv_filename = f\"nlweb_mcp_enhanced_metrics_{current_timestamp}.csv\"\n",
    "\n",
    "\n",
    "    # Prepare CSV data\n",
    "    csv_data = []\n",
    "    for result in enhanced_summary[\"results\"]:\n",
    "        \n",
    "        csv_row = {\n",
    "            \"task_category\": result.get(\"task_id\", \"\").split(\"_Task\")[0],\n",
    "            \"task_id\": result.get(\"task_id\", \"\"),\n",
    "            \"task_completion_rate\": result.get(\"task_completion_rate\", 0),\n",
    "            \"precision\": result.get(\"precision\", 0),\n",
    "            \"recall\": result.get(\"recall\", 0),\n",
    "            \"f1_score\": result.get(\"f1_score\", 0),\n",
    "            \"prompt_tokens\": result.get(\"prompt_tokens\", 0),\n",
    "            \"completion_tokens\": result.get(\"completion_tokens\", 0),\n",
    "            \"execution_time\": result.get(\"execution_time_seconds\", 0),\n",
    "        }\n",
    "        csv_data.append(csv_row)\n",
    "\n",
    "    # Write CSV file\n",
    "    try:\n",
    "        with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = [\"task_category\",\"task_id\", \"task_completion_rate\", \"precision\",\n",
    "                          \"recall\", \"f1_score\", \"prompt_tokens\", \"completion_tokens\", \"execution_time\"]\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(csv_data)\n",
    "\n",
    "        print(f\"📊 CSV metrics exported to: {csv_filename}\")\n",
    "        return csv_filename\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to generate CSV: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 CSV metrics exported to: nlweb_mcp_enhanced_metrics_20250725_104451.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nlweb_mcp_enhanced_metrics_20250725_104451.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../results/v1/mcp/gpt4.1/hybrid_execution_history_20250725_104451.json\", \"r\") as f:\n",
    "    enhanced_summary = json.load(f)\n",
    "\n",
    "generate_csv_metrics(enhanced_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Configuration: your real shop URLs and solution page URL\n",
    "URLS = {\n",
    "    \"URL_1\": \"https://webmall-1.informatik.uni-mannheim.de\",\n",
    "    \"URL_2\": \"https://webmall-2.informatik.uni-mannheim.de\",\n",
    "    \"URL_3\": \"https://webmall-3.informatik.uni-mannheim.de\",\n",
    "    \"URL_4\": \"https://webmall-4.informatik.uni-mannheim.de\"\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    \"\"\"Normalize URL for comparison by removing trailing slashes and converting to lowercase.\"\"\"\n",
    "    return url.rstrip('/').lower()\n",
    "\n",
    "\n",
    "def fill_urls(text_or_list, urls: Dict[str, str]):\n",
    "    def replace_in_text(text: str) -> str:\n",
    "        for key, val in urls.items():\n",
    "            text = text.replace(\"{{\" + key + \"}}\", val)\n",
    "        return text\n",
    "\n",
    "    if isinstance(text_or_list, list):\n",
    "        return [replace_in_text(t) for t in text_or_list]\n",
    "    else:\n",
    "        return replace_in_text(text_or_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculation_results(benchmark_solutions, model_solution):\n",
    "    \"\"\"\n",
    "    Calculate task completion, precision, and recall metrics.\n",
    "    \n",
    "    Args:\n",
    "        benchmark_solutions: List of sets containing benchmark solutions\n",
    "        model_solution: List of sets containing model solution\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains task_completion_rate, avg_precision, avg_recall, f1_score\n",
    "    \"\"\"\n",
    "    if len(benchmark_solutions) != len(model_solution):\n",
    "        raise ValueError(\"benchmark_solutions and model_solution must have the same length\")\n",
    "    \n",
    "    if len(benchmark_solutions) == 0:\n",
    "        return {\n",
    "            'task_completion_rate': 0.0,\n",
    "            'avg_precision': 0.0,\n",
    "            'avg_recall': 0.0,\n",
    "            'f1_score': 0.0\n",
    "        }\n",
    "\n",
    "    task_completions = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for benchmark_set, model_set in zip(benchmark_solutions, model_solution):\n",
    "        # Convert to sets if they aren't already\n",
    "        benchmark_set = set(benchmark_set) if benchmark_set is not None else set()\n",
    "        model_set = set(model_set) if model_set is not None else set()\n",
    "\n",
    "        # Task completion: 1 if exact match, 0 otherwise\n",
    "        task_completion = 1 if benchmark_set == model_set else 0\n",
    "        task_completions.append(task_completion)\n",
    "\n",
    "        # Precision: intersection / model_set size\n",
    "        if len(model_set) > 0:\n",
    "            precision = len(benchmark_set.intersection(model_set)) / len(model_set)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "        precisions.append(precision)\n",
    "\n",
    "        # Recall: intersection / benchmark_set size\n",
    "        if len(benchmark_set) > 0:\n",
    "            recall = len(benchmark_set.intersection(model_set)) / len(benchmark_set)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        recalls.append(recall)\n",
    "\n",
    "    # Calculate aggregated metrics\n",
    "    task_completion_rate = sum(task_completions) / len(benchmark_solutions)\n",
    "    avg_precision = sum(precisions) / len(benchmark_solutions)\n",
    "    avg_recall = sum(recalls) / len(benchmark_solutions)\n",
    "    \n",
    "    # Calculate F1 score with zero division protection\n",
    "    if avg_precision + avg_recall > 0:\n",
    "        f1_score = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall)\n",
    "    else:\n",
    "        f1_score = 0.0\n",
    "\n",
    "    return {\n",
    "        'task_completion_rate': task_completion_rate,\n",
    "        'avg_precision': avg_precision,\n",
    "        'avg_recall': avg_recall,\n",
    "        'f1_score': f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"./rag/v2/results/v2/benchmark_v2_improved_results_20250722_215549.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>task_id</th>\n",
       "      <th>user_task</th>\n",
       "      <th>metrics</th>\n",
       "      <th>parsed_urls</th>\n",
       "      <th>db_urls_found</th>\n",
       "      <th>db_urls_missing</th>\n",
       "      <th>db_coverage</th>\n",
       "      <th>tool_history</th>\n",
       "      <th>total_searches</th>\n",
       "      <th>...</th>\n",
       "      <th>additional_urls</th>\n",
       "      <th>missing_urls</th>\n",
       "      <th>parsed_model_response</th>\n",
       "      <th>model_response</th>\n",
       "      <th>task_category</th>\n",
       "      <th>evaluation_urls</th>\n",
       "      <th>cart_checkout_urls</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>Webmall_Single_Product_Search_Task1</td>\n",
       "      <td>\\nFind all offers for the AMD Ryzen 9 5900X.\\n</td>\n",
       "      <td>{'task_completion_rate': 1, 'avg_precision': 1...</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-2.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'tool_name': 'search_products', 'tool_args':...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[\\n  \"https://webmall-1.informatik.uni-mannhei...</td>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>15379</td>\n",
       "      <td>110</td>\n",
       "      <td>15489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>Webmall_Single_Product_Search_Task2</td>\n",
       "      <td>\\nFind all offers for the Canon EOS R5 Mark II.\\n</td>\n",
       "      <td>{'task_completion_rate': 1, 'avg_precision': 1...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-2.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'tool_name': 'search_products', 'tool_args':...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[\\n  \"https://webmall-3.informatik.uni-mannhei...</td>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>29566</td>\n",
       "      <td>259</td>\n",
       "      <td>29825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>Webmall_Single_Product_Search_Task3</td>\n",
       "      <td>\\nFind all offers for the Samsung Galaxy A25 a...</td>\n",
       "      <td>{'task_completion_rate': 1, 'avg_precision': 1...</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'tool_name': 'search_products', 'tool_args':...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[\\n  \"https://webmall-1.informatik.uni-mannhei...</td>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>60841</td>\n",
       "      <td>493</td>\n",
       "      <td>61334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>Webmall_Single_Product_Search_Task4</td>\n",
       "      <td>\\nFind all offers for the Asus ROG Ryujin II A...</td>\n",
       "      <td>{'task_completion_rate': 1, 'avg_precision': 1...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'tool_name': 'search_products', 'tool_args':...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[\\n  \"https://webmall-3.informatik.uni-mannhei...</td>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>76235</td>\n",
       "      <td>654</td>\n",
       "      <td>76889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>Webmall_Single_Product_Search_Task5</td>\n",
       "      <td>\\nFind all offers for the Kingston 1TB NV2 M.2...</td>\n",
       "      <td>{'task_completion_rate': 1, 'avg_precision': 1...</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'tool_name': 'search_products', 'tool_args':...</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[\\n  \"https://webmall-1.informatik.uni-mannhei...</td>\n",
       "      <td>Webmall_Single_Product_Search</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>93929</td>\n",
       "      <td>798</td>\n",
       "      <td>94727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        category                              task_id  \\\n",
       "0  Webmall_Single_Product_Search  Webmall_Single_Product_Search_Task1   \n",
       "1  Webmall_Single_Product_Search  Webmall_Single_Product_Search_Task2   \n",
       "2  Webmall_Single_Product_Search  Webmall_Single_Product_Search_Task3   \n",
       "3  Webmall_Single_Product_Search  Webmall_Single_Product_Search_Task4   \n",
       "4  Webmall_Single_Product_Search  Webmall_Single_Product_Search_Task5   \n",
       "\n",
       "                                           user_task  \\\n",
       "0     \\nFind all offers for the AMD Ryzen 9 5900X.\\n   \n",
       "1  \\nFind all offers for the Canon EOS R5 Mark II.\\n   \n",
       "2  \\nFind all offers for the Samsung Galaxy A25 a...   \n",
       "3  \\nFind all offers for the Asus ROG Ryujin II A...   \n",
       "4  \\nFind all offers for the Kingston 1TB NV2 M.2...   \n",
       "\n",
       "                                             metrics  \\\n",
       "0  {'task_completion_rate': 1, 'avg_precision': 1...   \n",
       "1  {'task_completion_rate': 1, 'avg_precision': 1...   \n",
       "2  {'task_completion_rate': 1, 'avg_precision': 1...   \n",
       "3  {'task_completion_rate': 1, 'avg_precision': 1...   \n",
       "4  {'task_completion_rate': 1, 'avg_precision': 1...   \n",
       "\n",
       "                                         parsed_urls  \\\n",
       "0  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "1  [https://webmall-3.informatik.uni-mannheim.de/...   \n",
       "2  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "3  [https://webmall-3.informatik.uni-mannheim.de/...   \n",
       "4  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "\n",
       "                                       db_urls_found db_urls_missing  \\\n",
       "0  [https://webmall-2.informatik.uni-mannheim.de/...              []   \n",
       "1  [https://webmall-2.informatik.uni-mannheim.de/...              []   \n",
       "2  [https://webmall-1.informatik.uni-mannheim.de/...              []   \n",
       "3  [https://webmall-3.informatik.uni-mannheim.de/...              []   \n",
       "4  [https://webmall-3.informatik.uni-mannheim.de/...              []   \n",
       "\n",
       "   db_coverage                                       tool_history  \\\n",
       "0          1.0  [{'tool_name': 'search_products', 'tool_args':...   \n",
       "1          1.0  [{'tool_name': 'search_products', 'tool_args':...   \n",
       "2          1.0  [{'tool_name': 'search_products', 'tool_args':...   \n",
       "3          1.0  [{'tool_name': 'search_products', 'tool_args':...   \n",
       "4          1.0  [{'tool_name': 'search_products', 'tool_args':...   \n",
       "\n",
       "   total_searches  ... additional_urls  missing_urls  \\\n",
       "0               1  ...              []            []   \n",
       "1               1  ...              []            []   \n",
       "2               2  ...              []            []   \n",
       "3               1  ...              []            []   \n",
       "4               1  ...              []            []   \n",
       "\n",
       "                               parsed_model_response  \\\n",
       "0  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "1  [https://webmall-3.informatik.uni-mannheim.de/...   \n",
       "2  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "3  [https://webmall-3.informatik.uni-mannheim.de/...   \n",
       "4  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "\n",
       "                                      model_response  \\\n",
       "0  [\\n  \"https://webmall-1.informatik.uni-mannhei...   \n",
       "1  [\\n  \"https://webmall-3.informatik.uni-mannhei...   \n",
       "2  [\\n  \"https://webmall-1.informatik.uni-mannhei...   \n",
       "3  [\\n  \"https://webmall-3.informatik.uni-mannhei...   \n",
       "4  [\\n  \"https://webmall-1.informatik.uni-mannhei...   \n",
       "\n",
       "                   task_category  \\\n",
       "0  Webmall_Single_Product_Search   \n",
       "1  Webmall_Single_Product_Search   \n",
       "2  Webmall_Single_Product_Search   \n",
       "3  Webmall_Single_Product_Search   \n",
       "4  Webmall_Single_Product_Search   \n",
       "\n",
       "                                     evaluation_urls  cart_checkout_urls  \\\n",
       "0  [https://webmall-1.informatik.uni-mannheim.de/...                  []   \n",
       "1  [https://webmall-3.informatik.uni-mannheim.de/...                  []   \n",
       "2  [https://webmall-1.informatik.uni-mannheim.de/...                  []   \n",
       "3  [https://webmall-3.informatik.uni-mannheim.de/...                  []   \n",
       "4  [https://webmall-1.informatik.uni-mannheim.de/...                  []   \n",
       "\n",
       "   prompt_tokens  completion_tokens total_tokens  \n",
       "0          15379                110        15489  \n",
       "1          29566                259        29825  \n",
       "2          60841                493        61334  \n",
       "3          76235                654        76889  \n",
       "4          93929                798        94727  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:    \n",
    "    df = pd.read_json(log_file)\n",
    "except Exception as e:\n",
    "    # read json file\n",
    "    with open(log_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data[\"results\"])\n",
    "\n",
    "# Create a category column based on the id, removing the Task part\n",
    "df[\"category\"] = df[\"task_id\"].str.replace(r'_Task\\d+$', '', regex=True)\n",
    "\n",
    "# Move the category column to the first position\n",
    "df = df[[\"category\"] + [col for col in df.columns if col != \"category\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = pd.read_json(\"../task_sets.json\")\n",
    "\n",
    "# Extract all task ids from the 'tasks' column (which is a list of dicts per row)\n",
    "task_rows = []\n",
    "for _, row in task_set.iterrows():\n",
    "    for task in row['tasks']:\n",
    "        task_rows.append({'task_id': task['id'], \"correct_answer\": task['correct_answer'][\"answers\"]})\n",
    "\n",
    "tasks_df = pd.DataFrame(task_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to robustly parse a JSON list of URLs from a string, even if there is extra data or formatting issues\n",
    "def safe_parse_url_list(s):\n",
    "    try:\n",
    "        # Try to parse as a single JSON list\n",
    "        result = json.loads(s)\n",
    "        if isinstance(result, list):\n",
    "            return set(normalize_url(url) for url in result)\n",
    "        # If not a list, try to extract the first list found in the string\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    # Fallback: try to extract the first JSON array from the string\n",
    "    import re\n",
    "    match = re.search(r'(\\[.*?\\])', s, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            result = json.loads(match.group(1))\n",
    "            if isinstance(result, list):\n",
    "                return set(normalize_url(url) for url in result)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # If all fails, return empty set\n",
    "    return set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume fill_url is defined elsewhere in the notebook\n",
    "tasks_df[\"correct_answer\"] = tasks_df[\"correct_answer\"].apply(lambda lst: fill_urls(lst, URLS))\n",
    "\n",
    "# Build a mapping from task_id to correct_answer for fast lookup\n",
    "taskid_to_answer = dict(zip(tasks_df[\"task_id\"], tasks_df[\"correct_answer\"]))\n",
    "\n",
    "# Add the column to df based on the id of the task, handling missing ids gracefully\n",
    "def get_correct_answer_for_id(task_id):\n",
    "    # Return the correct answer if found, else return an empty list\n",
    "    return taskid_to_answer.get(task_id, [])\n",
    "\n",
    "df[\"correct_answers\"] = df[\"task_id\"].apply(get_correct_answer_for_id)\n",
    "\n",
    "# parse model answers so that it is a list of strings\n",
    "if \"model_response\" in df.columns:\n",
    "    df[\"parser_model_answers\"] = df[\"model_response\"].apply(lambda lst: [normalize_url(url) for url in safe_parse_url_list(lst)])\n",
    "else:\n",
    "    df[\"parser_model_answers\"] = df[\"raw_response\"].apply(lambda lst: [normalize_url(url) for url in safe_parse_url_list(lst)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics for each row\n",
    "metrics = df.apply(lambda row: calculation_results([row[\"correct_answers\"]], [row[\"parser_model_answers\"]]), axis=1)\n",
    "# Convert metrics (a Series of dicts) to a DataFrame\n",
    "metrics_df = pd.DataFrame(list(metrics))\n",
    "\n",
    "# Compute per-row prompt_tokens and completion_tokens by differencing\n",
    "#prompt_tokens = df[\"prompt_tokens\"].astype(int).diff().fillna(df[\"prompt_tokens\"].iloc[0]).astype(int)\n",
    "#completion_tokens = df[\"completion_tokens\"].astype(int).diff().fillna(df[\"completion_tokens\"].iloc[0]).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# Prepare DataFrame in the requested order\n",
    "ordered_metrics = pd.DataFrame({\n",
    "    \"category\": df[\"category\"].reset_index(drop=True),\n",
    "    \"task_id\": df[\"task_id\"].reset_index(drop=True),\n",
    "    \"task_completion_rate\": metrics_df[\"task_completion_rate\"],\n",
    "    \"avg_precision\": metrics_df[\"avg_precision\"],\n",
    "    \"avg_recall\": metrics_df[\"avg_recall\"],\n",
    "    \"f1_score\": metrics_df[\"f1_score\"],\n",
    "    #\"prompt_tokens\": prompt_tokens.reset_index(drop=True),\n",
    "    #\"completion_tokens\": completion_tokens.reset_index(drop=True)\n",
    "    \"prompt_tokens\": df[\"prompt_tokens\"].reset_index(drop=True),\n",
    "    \"completion_tokens\": df[\"completion_tokens\"].reset_index(drop=True)\n",
    "})\n",
    "\n",
    "ordered_metrics.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>task_id</th>\n",
       "      <th>task_completion_rate</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>Webmall_Add_To_Cart_Task1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90062</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>Webmall_Add_To_Cart_Task2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119197</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>Webmall_Add_To_Cart_Task3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92469</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>Webmall_Add_To_Cart_Task4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159256</td>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>Webmall_Add_To_Cart_Task5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103487</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>Webmall_Add_To_Cart_Task6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99901</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>Webmall_Add_To_Cart_Task7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>91854</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2967</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2993</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112208</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>160461</td>\n",
       "      <td>597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>127811</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>45102</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>261478</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>Webmall_Checkout_Task8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>139896</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               category                    task_id  task_completion_rate  \\\n",
       "0   Webmall_Add_To_Cart  Webmall_Add_To_Cart_Task1                   0.0   \n",
       "1   Webmall_Add_To_Cart  Webmall_Add_To_Cart_Task2                   0.0   \n",
       "2   Webmall_Add_To_Cart  Webmall_Add_To_Cart_Task3                   0.0   \n",
       "3   Webmall_Add_To_Cart  Webmall_Add_To_Cart_Task4                   0.0   \n",
       "4   Webmall_Add_To_Cart  Webmall_Add_To_Cart_Task5                   0.0   \n",
       "5   Webmall_Add_To_Cart  Webmall_Add_To_Cart_Task6                   0.0   \n",
       "6   Webmall_Add_To_Cart  Webmall_Add_To_Cart_Task7                   0.0   \n",
       "7      Webmall_Checkout     Webmall_Checkout_Task1                   1.0   \n",
       "8      Webmall_Checkout     Webmall_Checkout_Task2                   1.0   \n",
       "9      Webmall_Checkout     Webmall_Checkout_Task3                   1.0   \n",
       "10     Webmall_Checkout     Webmall_Checkout_Task4                   1.0   \n",
       "11     Webmall_Checkout     Webmall_Checkout_Task5                   1.0   \n",
       "12     Webmall_Checkout     Webmall_Checkout_Task6                   1.0   \n",
       "13     Webmall_Checkout     Webmall_Checkout_Task7                   1.0   \n",
       "14     Webmall_Checkout     Webmall_Checkout_Task8                   1.0   \n",
       "\n",
       "    avg_precision  avg_recall  f1_score  prompt_tokens  completion_tokens  \n",
       "0             0.0         0.0       0.0          90062                220  \n",
       "1             0.0         0.0       0.0         119197                244  \n",
       "2             0.0         0.0       0.0          92469                236  \n",
       "3             0.0         0.0       0.0         159256                281  \n",
       "4             0.0         0.0       0.0         103487                212  \n",
       "5             0.0         0.0       0.0          99901                244  \n",
       "6             0.0         0.0       0.0          91854                212  \n",
       "7             1.0         1.0       1.0           2967                 83  \n",
       "8             1.0         1.0       1.0           2993                115  \n",
       "9             1.0         1.0       1.0         112208                476  \n",
       "10            1.0         1.0       1.0         160461                597  \n",
       "11            1.0         1.0       1.0         127811                492  \n",
       "12            1.0         1.0       1.0          45102                276  \n",
       "13            1.0         1.0       1.0         261478                562  \n",
       "14            1.0         1.0       1.0         139896                476  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/8f0zskws27x_14w3qmfndjfw0000gn/T/ipykernel_41171/405774577.py:1: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  category_metrics = df.groupby(\"category\").apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>task_completion_rate</th>\n",
       "      <th>avg_precision</th>\n",
       "      <th>avg_recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              category  task_completion_rate  avg_precision  avg_recall  \\\n",
       "0  Webmall_Add_To_Cart                   0.0            0.0         0.0   \n",
       "1     Webmall_Checkout                   1.0            1.0         1.0   \n",
       "\n",
       "   f1_score  \n",
       "0       0.0  \n",
       "1       1.0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_metrics = df.groupby(\"category\").apply(\n",
    "    lambda group: calculation_results(group[\"correct_answers\"], group[\"parser_model_answers\"])\n",
    ")\n",
    "\n",
    "# category_metrics is a Series with category as index and dicts as values.\n",
    "# Convert to DataFrame with dict keys as columns.\n",
    "results_df = pd.DataFrame(list(category_metrics.values), index=category_metrics.index).reset_index()\n",
    "results_df = results_df.rename(columns={'index': 'category'})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/8f0zskws27x_14w3qmfndjfw0000gn/T/ipykernel_41171/41579818.py:19: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  completion_rates = df.groupby('category').apply(\n"
     ]
    }
   ],
   "source": [
    "# Check if metrics are in separate columns or nested\n",
    "has_metrics_column = 'metrics' in df.columns\n",
    "has_separate_metrics = all(col in df.columns for col in ['accuracy', 'precision', 'recall'])\n",
    "\n",
    "if has_metrics_column and not has_separate_metrics:\n",
    "    # Original format with nested metrics\n",
    "    category_metrics = df.groupby('category').agg({\n",
    "        'metrics': lambda x: {\n",
    "            'accuracy': x.apply(lambda m: m['accuracy']).mean(),\n",
    "            'precision': x.apply(lambda m: m['precision']).mean(),\n",
    "            'recall': x.apply(lambda m: m['recall']).mean(),\n",
    "            'f1_score': x.apply(lambda m: m['f1_score']).mean()\n",
    "        },\n",
    "        'prompt_tokens': 'mean',\n",
    "        'completion_tokens': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate completion rate (percentage of tasks with accuracy = 1.0)\n",
    "    completion_rates = df.groupby('category').apply(\n",
    "        lambda x: (x['metrics'].apply(lambda m: m['accuracy']) == 1.0).mean()\n",
    "    ).reset_index()\n",
    "    completion_rates.columns = ['category', 'completion_rate']\n",
    "    \n",
    "    # Create a DataFrame with separate columns for each metric\n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\n",
    "            'category': row['category'],\n",
    "            'accuracy': row['metrics']['accuracy'],\n",
    "            'precision': row['metrics']['precision'],\n",
    "            'recall': row['metrics']['recall'],\n",
    "            'f1_score': row['metrics']['f1_score'],\n",
    "            'prompt_tokens': row['prompt_tokens'],\n",
    "            'completion_tokens': row['completion_tokens']\n",
    "        }\n",
    "        for _, row in category_metrics.iterrows()\n",
    "    ])\n",
    "    \n",
    "elif has_separate_metrics:\n",
    "    # New format with separate metric columns\n",
    "    agg_dict = {\n",
    "        'accuracy': 'mean',\n",
    "        'precision': 'mean', \n",
    "        'recall': 'mean',\n",
    "        'prompt_tokens': 'mean',\n",
    "        'completion_tokens': 'mean'\n",
    "    }\n",
    "    \n",
    "    # Add f1_score if it exists, otherwise calculate it\n",
    "    if 'f1_score' in df.columns:\n",
    "        agg_dict['f1_score'] = 'mean'\n",
    "    \n",
    "    # Add total_tokens if it exists\n",
    "    if 'total_tokens' in df.columns:\n",
    "        agg_dict['total_tokens'] = 'mean'\n",
    "    \n",
    "    category_metrics = df.groupby('category').agg(agg_dict).reset_index()\n",
    "    \n",
    "    # Calculate f1_score if it doesn't exist\n",
    "    if 'f1_score' not in category_metrics.columns:\n",
    "        category_metrics['f1_score'] = 2 * (category_metrics['precision'] * category_metrics['recall']) / (category_metrics['precision'] + category_metrics['recall'])\n",
    "        category_metrics['f1_score'] = category_metrics['f1_score'].fillna(0)  # Handle division by zero\n",
    "    \n",
    "    # Calculate completion rate (percentage of tasks with accuracy = 1.0)\n",
    "    completion_rates = df.groupby('category').apply(\n",
    "        lambda x: (x['accuracy'] == 1.0).mean()\n",
    "    ).reset_index()\n",
    "    completion_rates.columns = ['category', 'completion_rate']\n",
    "    \n",
    "    metrics_df = category_metrics\n",
    "else:\n",
    "    raise ValueError(\"Could not find metrics in expected format\")\n",
    "\n",
    "# Add NLWeb token usage if available\n",
    "if \"nlweb_token_usage\" in df.columns:\n",
    "    # Get all unique models across all categories\n",
    "    all_models = set()\n",
    "    for _, row in df.iterrows():\n",
    "        if 'nlweb_token_usage' in row and 'by_model' in row['nlweb_token_usage']:\n",
    "            all_models.update(row['nlweb_token_usage']['by_model'].keys())\n",
    "    \n",
    "    # Calculate average NLWeb token usage by category\n",
    "    nlweb_data = []\n",
    "    for category in metrics_df['category']:\n",
    "        category_data = df[df['category'] == category]\n",
    "        \n",
    "        # Initialize row data\n",
    "        row_data = {\n",
    "            'category': category,\n",
    "            'nlweb_total_calls': category_data['nlweb_token_usage'].apply(lambda n: n.get('total_calls', 0)).mean(),\n",
    "            'nlweb_total_prompt_tokens': category_data['nlweb_token_usage'].apply(lambda n: n.get('summary', {}).get('total_prompt_tokens', 0)).mean(),\n",
    "            'nlweb_total_completion_tokens': category_data['nlweb_token_usage'].apply(lambda n: n.get('summary', {}).get('total_completion_tokens', 0)).mean(),\n",
    "            'nlweb_total_tokens': category_data['nlweb_token_usage'].apply(lambda n: n.get('summary', {}).get('total_tokens', 0)).mean()\n",
    "        }\n",
    "        \n",
    "        # Add individual model token usage\n",
    "        for model in all_models:\n",
    "            model_prompt_tokens = []\n",
    "            model_completion_tokens = []\n",
    "            model_total_tokens = []\n",
    "            model_call_count = []\n",
    "            \n",
    "            for _, task_row in category_data.iterrows():\n",
    "                if ('nlweb_token_usage' in task_row and \n",
    "                    'by_model' in task_row['nlweb_token_usage'] and \n",
    "                    model in task_row['nlweb_token_usage']['by_model']):\n",
    "                    \n",
    "                    model_data = task_row['nlweb_token_usage']['by_model'][model]\n",
    "                    model_prompt_tokens.append(model_data.get('prompt_tokens', 0))\n",
    "                    model_completion_tokens.append(model_data.get('completion_tokens', 0))\n",
    "                    model_total_tokens.append(model_data.get('total_tokens', 0))\n",
    "                    model_call_count.append(model_data.get('call_count', 0))\n",
    "            \n",
    "            # Use the requested column naming convention\n",
    "            model_name_clean = model.replace('-', '_').replace('.', '_')\n",
    "            if model_prompt_tokens:  # Only add if we have data for this model\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_prompt'] = sum(model_prompt_tokens) / len(model_prompt_tokens)\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_output'] = sum(model_completion_tokens) / len(model_completion_tokens)\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_total'] = sum(model_total_tokens) / len(model_total_tokens)\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_calls'] = sum(model_call_count) / len(model_call_count)\n",
    "            else:\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_prompt'] = 0\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_output'] = 0\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_total'] = 0\n",
    "                row_data[f'nl_web_usage_{model_name_clean}_calls'] = 0\n",
    "        \n",
    "        nlweb_data.append(row_data)\n",
    "    \n",
    "    # Create NLWeb DataFrame and merge with main metrics\n",
    "    nlweb_df = pd.DataFrame(nlweb_data)\n",
    "    metrics_df = metrics_df.merge(nlweb_df, on='category', how='left')\n",
    "\n",
    "# Merge with completion rates\n",
    "metrics_df = metrics_df.merge(completion_rates, on='category')\n",
    "\n",
    "metrics_df\n",
    "\n",
    "# write to csv\n",
    "metrics_df.to_csv('metrics_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>completion_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webmall_Add_To_Cart</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.852381</td>\n",
       "      <td>108032.285714</td>\n",
       "      <td>235.571429</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Webmall_Checkout</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106614.500000</td>\n",
       "      <td>384.625000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              category  accuracy  precision    recall  f1_score  \\\n",
       "0  Webmall_Add_To_Cart  0.928571   0.809524  0.928571  0.852381   \n",
       "1     Webmall_Checkout  1.000000   1.000000  1.000000  1.000000   \n",
       "\n",
       "   prompt_tokens  completion_tokens  completion_rate  \n",
       "0  108032.285714         235.571429         0.857143  \n",
       "1  106614.500000         384.625000         1.000000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used by each model across all categories:\n",
      "gpt_4_1_mini: 581,329 tokens\n",
      "gpt_4_1: 14,576 tokens\n",
      "\n",
      "Total input tokens used by each model across all categories:\n",
      "gpt_4_1_mini: 518,246 tokens\n",
      "gpt_4_1: 12,763 tokens\n",
      "\n",
      "Total output tokens used by each model across all categories:\n",
      "gpt_4_1_mini: 63,083 tokens\n",
      "gpt_4_1: 1,812 tokens\n"
     ]
    }
   ],
   "source": [
    "# Sum up the total tokens for each model across all categories\n",
    "total_tokens_by_model = {}\n",
    "\n",
    "for _, row in metrics_df.iterrows():\n",
    "    for col in metrics_df.columns:\n",
    "        if col.startswith('nl_web_usage_') and col.endswith('_total'):\n",
    "            model_name = col.replace('nl_web_usage_', '').replace('_total', '')\n",
    "            if model_name not in total_tokens_by_model:\n",
    "                total_tokens_by_model[model_name] = 0\n",
    "            total_tokens_by_model[model_name] += row[col]\n",
    "\n",
    "print(\"Total tokens used by each model across all categories:\")\n",
    "for model, total_tokens in total_tokens_by_model.items():\n",
    "    print(f\"{model}: {total_tokens:,.0f} tokens\")\n",
    "\n",
    "# Sum up the input and output tokens for each model across all categories\n",
    "total_input_tokens_by_model = {}\n",
    "total_output_tokens_by_model = {}\n",
    "\n",
    "for _, row in metrics_df.iterrows():\n",
    "    for col in metrics_df.columns:\n",
    "        if col.startswith('nl_web_usage_') and col.endswith('_prompt'):\n",
    "            model_name = col.replace('nl_web_usage_', '').replace('_prompt', '')\n",
    "            if model_name not in total_input_tokens_by_model:\n",
    "                total_input_tokens_by_model[model_name] = 0\n",
    "            total_input_tokens_by_model[model_name] += row[col]\n",
    "        elif col.startswith('nl_web_usage_') and col.endswith('_output'):\n",
    "            model_name = col.replace('nl_web_usage_', '').replace('_output', '')\n",
    "            if model_name not in total_output_tokens_by_model:\n",
    "                total_output_tokens_by_model[model_name] = 0\n",
    "            total_output_tokens_by_model[model_name] += row[col]\n",
    "\n",
    "print(\"\\nTotal input tokens used by each model across all categories:\")\n",
    "for model, total_tokens in total_input_tokens_by_model.items():\n",
    "    print(f\"{model}: {total_tokens:,.0f} tokens\")\n",
    "\n",
    "print(\"\\nTotal output tokens used by each model across all categories:\")\n",
    "for model, total_tokens in total_output_tokens_by_model.items():\n",
    "    print(f\"{model}: {total_tokens:,.0f} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool Call Statistics:\n",
      "            tool_name                   mcp_server  count  successful_results  empty_results\n",
      "        ask_webmall_4    WebMall-4 (Hardware Cafe)     10                   0             10\n",
      "        ask_webmall_2         WebMall-2 (TechTalk)     10                   0             10\n",
      "        ask_webmall_1 WebMall-1 (E-Store Athletes)      9                   0              9\n",
      "add_to_cart_webmall_4    WebMall-4 (Hardware Cafe)      9                   0              9\n",
      "add_to_cart_webmall_1 WebMall-1 (E-Store Athletes)      9                   0              9\n",
      "        ask_webmall_3       WebMall-3 (CamelCases)      8                   0              8\n",
      "add_to_cart_webmall_2         WebMall-2 (TechTalk)      8                   0              8\n",
      "add_to_cart_webmall_3       WebMall-3 (CamelCases)      7                   0              7\n",
      "   checkout_webmall_4    WebMall-4 (Hardware Cafe)      7                   0              7\n",
      "   checkout_webmall_2         WebMall-2 (TechTalk)      5                   0              5\n",
      "   checkout_webmall_1 WebMall-1 (E-Store Athletes)      3                   0              3\n",
      "   checkout_webmall_3       WebMall-3 (CamelCases)      2                   0              2\n",
      "  view_cart_webmall_1 WebMall-1 (E-Store Athletes)      2                   0              2\n",
      "  view_cart_webmall_2         WebMall-2 (TechTalk)      2                   0              2\n",
      "  view_cart_webmall_4    WebMall-4 (Hardware Cafe)      2                   0              2\n",
      "get_product_webmall_3       WebMall-3 (CamelCases)      2                   0              2\n",
      "get_product_webmall_1 WebMall-1 (E-Store Athletes)      1                   0              1\n",
      " reset_cart_webmall_4    WebMall-4 (Hardware Cafe)      1                   0              1\n",
      " reset_cart_webmall_1 WebMall-1 (E-Store Athletes)      1                   0              1\n",
      "  view_cart_webmall_3       WebMall-3 (CamelCases)      1                   0              1\n",
      "\n",
      "Total tool calls: 99\n",
      "Unique tools used: 20\n"
     ]
    }
   ],
   "source": [
    "# Generate a tool call statistics table\n",
    "import json\n",
    "\n",
    "# Extract all tool calls from all tasks\n",
    "all_tool_calls = []\n",
    "for _, row in df.iterrows():\n",
    "    all_tool_calls.extend(row['tool_calls'])\n",
    "\n",
    "# Count tool usage by name and server\n",
    "tool_stats = {}\n",
    "for tool_call in all_tool_calls:\n",
    "    tool_name = tool_call['tool_name']\n",
    "    mcp_server = tool_call['mcp_server']\n",
    "    \n",
    "    key = f\"{tool_name} ({mcp_server})\"\n",
    "    if key not in tool_stats:\n",
    "        tool_stats[key] = {\n",
    "            'tool_name': tool_name,\n",
    "            'mcp_server': mcp_server,\n",
    "            'count': 0,\n",
    "            'successful_results': 0,\n",
    "            'empty_results': 0\n",
    "        }\n",
    "    \n",
    "    tool_stats[key]['count'] += 1\n",
    "    \n",
    "    # Check if the tool call returned results\n",
    "    try:\n",
    "        output = json.loads(tool_call['tool_output'])\n",
    "        if 'products' in output and output['products']:\n",
    "            tool_stats[key]['successful_results'] += 1\n",
    "        elif 'items' in output and output['items']:\n",
    "            tool_stats[key]['successful_results'] += 1\n",
    "        elif 'ecommerce_items' in output and output['ecommerce_items']:\n",
    "            tool_stats[key]['successful_results'] += 1\n",
    "        elif 'products_stock_info' in output and output['products_stock_info']:\n",
    "            tool_stats[key]['successful_results'] += 1\n",
    "        else:\n",
    "            tool_stats[key]['empty_results'] += 1\n",
    "    except:\n",
    "        tool_stats[key]['empty_results'] += 1\n",
    "\n",
    "# Create DataFrame for tool statistics\n",
    "tool_stats_df = pd.DataFrame.from_dict(tool_stats, orient='index')\n",
    "tool_stats_df = tool_stats_df.reset_index(drop=True)\n",
    "tool_stats_df = tool_stats_df.sort_values('count', ascending=False)\n",
    "\n",
    "\n",
    "print(\"Tool Call Statistics:\")\n",
    "print(tool_stats_df.to_string(index=False))\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nTotal tool calls: {len(all_tool_calls)}\")\n",
    "print(f\"Unique tools used: {len(tool_stats_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tool_name</th>\n",
       "      <th>mcp_server</th>\n",
       "      <th>count</th>\n",
       "      <th>successful_results</th>\n",
       "      <th>empty_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ask_webmall_1</td>\n",
       "      <td>WebMall-1 (E-Store Athletes)</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ask_webmall_2</td>\n",
       "      <td>WebMall-2 (TechTalk)</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ask_webmall_3</td>\n",
       "      <td>WebMall-3 (CamelCases)</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ask_webmall_4</td>\n",
       "      <td>WebMall-4 (Hardware Cafe)</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>get_product_webmall_3</td>\n",
       "      <td>WebMall-3 (CamelCases)</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>get_product_webmall_1</td>\n",
       "      <td>WebMall-1 (E-Store Athletes)</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tool_name                    mcp_server  count  \\\n",
       "0          ask_webmall_1  WebMall-1 (E-Store Athletes)     74   \n",
       "1          ask_webmall_2          WebMall-2 (TechTalk)     74   \n",
       "2          ask_webmall_3        WebMall-3 (CamelCases)     74   \n",
       "3          ask_webmall_4     WebMall-4 (Hardware Cafe)     74   \n",
       "5  get_product_webmall_3        WebMall-3 (CamelCases)      3   \n",
       "4  get_product_webmall_1  WebMall-1 (E-Store Athletes)      2   \n",
       "\n",
       "   successful_results  empty_results  \n",
       "0                   0             74  \n",
       "1                   0             74  \n",
       "2                   0             74  \n",
       "3                   0             74  \n",
       "5                   0              3  \n",
       "4                   0              2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis\n",
    "\n",
    "This section analyzes three types of errors across different evaluation approaches:\n",
    "1. Cases where the retrieval system found all correct solutions but the model didn't return all  \n",
    "2. Cases where the model found additional (incorrect) solutions\n",
    "3. Cases where the retrieval system didn't retrieve all correct solutions\n",
    "\n",
    "The analysis adapts to different data formats and metrics available (recall vs RAG efficiency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ERROR ANALYSIS ===\n",
      "Data format detected: MCP-based evaluation\n",
      "\n",
      "=== CASE 1: MCP found all solutions but model missed some ===\n",
      "Total cases: 15\n",
      "\n",
      "Detailed analysis:\n",
      "\n",
      "Task ID: Webmall_Single_Product_Search_Task3\n",
      "Task: \\nFind all offers for the Samsung Galaxy A25 and A35.\\n...\n",
      "MCP Recall: 100% | Model Accuracy: 75%\n",
      "Missing URLs: ['https://webmall-2.informatik.uni-mannheim.de/product/samsung-galaxy-a35-5g-a356b-dual-sim-256gb-awesome-iceblue-android-14-smartphone']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/smartphone-samsung-galaxy-a25-5g-6-128gb-yellow-sm-a256bzydeue/\",\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-galaxy-a25-5g-a256b-dual-sim-128gb-blue-black-android-14-0-smartphone/\",\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-galaxy-a25-5g-a256b-dual-sim-128gb-blue-android-14-0-smartphone/\",\"https://webmall-3.informatik.uni-mannheim.de/product/samsung-galaxy-a35-5g-a356b-dual-sim-256gb-awesome-iceblue-android-14-smartphone/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Single_Product_Search_Task6\n",
      "Task: \\nFind all offers for a Corsair Vengeance 32GB Kit (2 x 16GB) DDR5.\\n...\n",
      "MCP Recall: 100% | Model Accuracy: 64%\n",
      "Missing URLs: ['https://webmall-1.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-6200mhz-pc5-49600-cl36-1-4v-xmp-3-0-pmic-dimm-memory', 'https://webmall-1.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-5600mhz-pc5-44800-cl40-1-25v-xmp-3-0-pmic-dimm-memory-black', 'https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-5600mhz-pc5-44800-cl36-1-25v-xmp-3-0-pmic-dimm-memory', 'https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-6000mhz-pc5-48000-cl36-1-25v-xmp-3-0-pmic-dimm-memory-white']\n",
      "Model response: [\n",
      "  \"https://webmall-1.informatik.uni-mannheim.de/product/corsair-vengeance-32gb-kit-2-x-16gb-ddr5-5200mhz-pc5-41600-cl40-1-25v-amd-optimised-pmic-dimm-memory/\",\n",
      "  \"https://webmall-1.informatik.uni-mannheim.de/product/corsair-vengeance-32gb-kit-2-x-16gb-ddr5-5600mhz-pc5-44800-cl36-1-25v-pmic-amd-optimised-dimm-memory/\",\n",
      "  \"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-32gb-kit-2-x-16gb-ddr5-6400mhz-pc5-51200-cl32-1-4v-xmp-3-0-pmic-dimm-memory/\",\n",
      "  \"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-32gb-kit-2-x-16gb-ddr5-5600mhz-pc5-44800-cl36-1-25v-pmic-amd-optimised-dimm-memory/\",\n",
      "  \"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-32gb-kit-2-x-16gb-ddr5-5200mhz-pc5-41600-cl40-1-25v-xmp-3-0-pmic-dimm-memory/\",\n",
      "  \"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-32gb-kit-2-x-16gb-ddr5-5600mhz-pc5-44800-cl36-1-25v-xmp-3-0-pmic-amd-optimised-dimm-memory/\",\n",
      "  \"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-32gb-kit-2-x-16gb-ddr5-6200mhz-pc5-49600-cl36-1-4v-xmp-3-0-dimm-memory/\",\n",
      "  \"https://webmall-2.informatik.uni-mannheim.de/product/ram-ddr43200-32gb-16gbx2-corsair-vengeance-lpx-black-cmk32gx4m2e3200c16/\"\n",
      "]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Single_Product_Search_Task12\n",
      "Task: \\nFind all offers for the ASUS ROG FALCHION ACE Compact 65% Keyboard.\\n...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-3.informatik.uni-mannheim.de/product/asus-rog-falchion-ace-compact-65-mechanical-rgb-gaming-keyboard-wired-dual-usb-c-rog-nx-red-switches-per-key-rgb-lighting-touch-panel', 'https://webmall-1.informatik.uni-mannheim.de/product/asus-rog-falchion-ace-compact-65-mechanical-rgb-gaming-keyboard-wired-dual-usb-c-rog-nx-red-switches-per-key-rgb-lighting-touch-panel', 'https://webmall-1.informatik.uni-mannheim.de/product/asus-rog-falchion-ace-compact-65-mechanical-rgb-gaming-keyboard-wired-dual-usb-c-rog-nx-red-switches-per-key-rgb-lighting-touch-panel-white-edition']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/asus-rog-falchion-ace-compact-65-mechanical-rgb-gaming-keyboard-wired-dual-usb-c-rog-nx-red-switches-per-key-rgb-lighting-touch-panel/\",\"https://webmall-3.informatik.uni-mannheim.de/product/asus-rog-falchion-ace-compact-65-mechanical-rgb-gaming-keyboard-wired-dual-usb-c-rog-nx-red-switches-per-key-rgb-lighting-touch-panel/\"]\n",
      "[\"https://webmall-1.informatik.uni-mannheim.de/product/asus-rog-falchion-ace-compact-65-mechanical-rgb-gaming-keyboard-wired-dual-usb-c-rog-nx-red-switches-per-key-rgb-lighting-touch-panel/\",\"https://webmall-3.informatik.uni-mannheim.de/product/asus-rog-falchion-ace-compact-65-mechanical-rgb-gaming-keyboard-wired-dual-usb-c-rog-nx-red-switches-per-key-rgb-lighting-touch-panel/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Cheapest_Product_Search_Task1\n",
      "Task: \\nFind the cheapest offer for the Bose Quietcomfort Ultra Headphones. If multiple offers share the l...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-3.informatik.uni-mannheim.de/product/bose-quietcomfort-ultra-headphone', 'https://webmall-4.informatik.uni-mannheim.de/product/bose-quietcomfort-ultra-headphones-black']\n",
      "Model response: [\"https://webmall-4.informatik.uni-mannheim.de/product/bose-quietcomfort-45-noise-canceling-wireless-over-ear-headphones/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Cheapest_Product_Search_Task2\n",
      "Task: \\nFind the cheapest offer for each storage size of the Samsung 870 Evo. If multiple offers share the...\n",
      "MCP Recall: 100% | Model Accuracy: 75%\n",
      "Missing URLs: ['https://webmall-3.informatik.uni-mannheim.de/product/samsung-4tb-870-evo-ssd-2-5-sata3-v-nand-r-w-560-530-mb-s-98k-88k-iops-7mm']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/samsung-870-evo-2-5-250-gb-sata-iii-v-nand/\",\"https://webmall-3.informatik.uni-mannheim.de/product/hard-disk-25-ssd-1tb-samsung-solid-state-870-evo-series-mz-77e1t0b-eu/\",\"https://webmall-4.informatik.uni-mannheim.de/product/samsung-ssd-870-evo-1tb/\",\"https://webmall-1.informatik.uni-mannheim.de/product/samsung-1tb-870-evo-ssd-2-5-sata3-v-nand-r-w-560-530-mb-s-98k-88k-iops-7mm/\",\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-ssd-870-evo-series-1tb-sataiii-2-5-r560mb-s-w530mb-s-6-8mm-basic-pack/\",\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-870-evo-2tb-2-5-sata-iii-6gb-s-ssd-560r-530w-mb-s-mz-77e2t0bw/\",\"https://webmall-3.informatik.uni-mannheim.de/product/samsung-870-evo-2tb-ssd-drive/\",\"https://webmall-4.informatik.uni-mannheim.de/product/samsung-870-evo-2-5-4000-gb-sata-iii-v-nand/\",\"https://webmall-3.informatik.uni-mannheim.de/product/ssd-samsung-870-evo-4tb-sata-iii/\",\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-870-evo-2-5-4-tb-serial-ata-iii-v-nand/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Best_Fit_Vague_Task4\n",
      "Task: \\nFind all offers for DDR5 RAM that match a colorful LED gaming aesthetic.\\n...\n",
      "MCP Recall: 100% | Model Accuracy: 67%\n",
      "Missing URLs: ['https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-6000mhz-pc5-48000-cl36-1-25v-xmp-3-0-pmic-dimm-memory-white', 'https://webmall-3.informatik.uni-mannheim.de/product/corsair-dominator-platinum-rgb-32gb-kit-2-x-16gb-ddr5-6200mhz-pc5-49600-cl36-1-3v-xmp-3-0-pmic-dimm-memory']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-5600mhz-pc5-44800-cl40-1-25v-xmp-3-0-pmic-dimm-memory-black/\",\"https://webmall-1.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-6200mhz-pc5-49600-cl36-1-4v-xmp-3-0-pmic-dimm-memory/\",\"https://webmall-3.informatik.uni-mannheim.de/product/kingston-fury-beast-rgb-32gb-kit-2-x-16gb-ddr5-6000mhz-pc5-48000-cl36-1-35v-ecc-xmp-3-0-pmic-amd-expo-dimm-memory/\",\"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-64gb-kit-4-x-16gb-ddr5-5600mhz-pc5-44800-cl36-1-25v-xmp-3-0-pmic-dimm-memory/\",\"https://webmall-3.informatik.uni-mannheim.de/product/corsair-dominator-platinum-rgb-32gb-kit-2-x-16gb-ddr5-6200mhz-pc5-49600-cl36-1-4v-xmp-3-0-pmic-dimm-memory/\",\"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-rgb-32gb-kit-2-x-16gb-ddr5-5600mhz-pc5-44800-cl36-1-25v-xmp-3-0-pmic-dimm-memory/\",\"https://webmall-4.informatik.uni-mannheim.de/product/adata-xpg-lancer-blade-rgb-32gb-kit-2-x-16gb-ddr5-6000mhz-cl30-1-35v-ecc-pmic-xmp-3-0-amd-expo-dimm-memory/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Cheapest_Best_Fit_Specific_Task1\n",
      "Task: \\nFind the cheapest offer for a Xbox gaming console with at least 512gb disk space in black. If mult...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-3.informatik.uni-mannheim.de/product/microsoft-xbox-series-s-starter-bundle-xbox-series-s-hvit-10240-mb-gddr6-amd-amd-ryzen-zen-2']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/microsoft-xbox-series-x-gaming-console-1tb-black-platform-for-display-xbox-series-x-edition-standard/\",\"https://webmall-4.informatik.uni-mannheim.de/product/microsoft-xbox-one-x-1tb-console-black/\",\"https://webmall-3.informatik.uni-mannheim.de/product/microsoft-xbox-one-x-1tb-4k-ultra-hd-gaming-console-black-renewed-2017-model/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Cheapest_Best_Fit_Specific_Task2\n",
      "Task: \\nFind the cheapest offer for a Samsung Galaxy smartphone from the S24 series which has a camera wit...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-4.informatik.uni-mannheim.de/product/smartphone-samsung-galaxy-s24-ultra-5g-12-256gb-black-titanium-black-sm-s928bzkgeue']\n",
      "Model response: [\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-galaxy-s24-ultra-5g-s928b-ds-256gb-titanium-black-android-14-smartphone/\",\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-galaxy-s24-ultra-5g-s928b-ds-256gb-titanium-gray-android-14-smartphone/\",\"https://webmall-2.informatik.uni-mannheim.de/product/samsung-galaxy-s24-ultra-5g-s928b-ds-256gb-titanium-yellow-android-14-smartphone/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Cheapest_Best_Fit_Specific_Task8\n",
      "Task: \\nFind the cheapest offer a PC PSU with more than 550 W. If multiple offers share the lowest price, ...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-2.informatik.uni-mannheim.de/product/fractal-design-560w-ion-2-psu-fully-modular-fluid-dynamic-fan-80-platinum-ultraflex-wires-whisper-quiet-mode']\n",
      "Model response: [\"https://webmall-2.informatik.uni-mannheim.de/product/gigabyte-p550b-power-supply-unit-550-w-204-pin-atx-atx-black/\",\"https://webmall-4.informatik.uni-mannheim.de/product/gamemax-550w-gp550-white-psu-fully-wired-80-bronze-power-lead-not-included/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Cheapest_Best_Fit_Vague_Task2\n",
      "Task: \\nFind the cheapest offer for an external SSD that is not colored black, white or metallic. If multi...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-2.informatik.uni-mannheim.de/product/adata-se880-1tb-pocket-size-external-ssd-usb-3-2-gen2-type-c-type-a-blue']\n",
      "Model response: [\"https://webmall-2.informatik.uni-mannheim.de/product/adata-sc610-1tb-pocket-size-external-ssd-usb-3-2-gen2-type-a-capless-retractable-design-key-ring/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Cheapest_Best_Fit_Vague_Task6\n",
      "Task: \\nFind the cheapest offer for a large PC case from ASUS that can be easily carried to frequent LAN p...\n",
      "MCP Recall: 100% | Model Accuracy: 67%\n",
      "Missing URLs: ['https://webmall-4.informatik.uni-mannheim.de/product/asus-tuf-gaming-gt501-white-gaming-case-w-window-e-atx-tempered-smoked-glass-3-x-12cm-rgb-fans-carry-handles']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/asus-tuf-gaming-gt501-gaming-case-w-window-e-atx-tempered-smoked-glass-3-x-12cm-rgb-fans-carry-handles/\",\"https://webmall-4.informatik.uni-mannheim.de/product/asus-tuf-gaming-gt501-gaming-case-w-window-e-atx-tempered-smoked-glass-3-x-12cm-rgb-fans-carry-handles/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Find_Compatible_Products_Task5\n",
      "Task: \\nFind an HDMI cable to connect my monitor to this GPU: https://webmall-1.informatik.uni-mannheim.de...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-4.informatik.uni-mannheim.de/product/sandberg-mini-displayport-male-to-hdmi-female-converter-cable-white-5-year-warranty']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/hama-high-speed-hdmi-cable-w-rotatable-plugs-male-to-male-1-5-metre/\",\"https://webmall-1.informatik.uni-mannheim.de/product/hama-high-speed-hdmi-cable-ultra-hd-4k-ethernet-gold-plated-5-metre/\",\"https://webmall-1.informatik.uni-mannheim.de/product/hama-high-speed-hdmi-cable-3-metres-supports-4k/\",\"https://webmall-2.informatik.uni-mannheim.de/product/hama-high-speed-hdmi-cable-3-metres-supports-4k/\",\"https://webmall-3.informatik.uni-mannheim.de/product/high-speed-hdmi-cable-3/\",\"https://webmall-4.informatik.uni-mannheim.de/product/hama-high-speed-hdmi-cable-3-metres-supports-4k/\",\"https://webmall-4.informatik.uni-mannheim.de/product/hama-high-speed-hdmi-cable-ultra-hd-4k-ethernet-gold-plated-5-metre/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_EndToEnd_Task4\n",
      "Task: \\nFind the cheapest offer for the Asus DUAL RTX4070 SUPER OC White, add it to the shopping cart and ...\n",
      "MCP Recall: 100% | Model Accuracy: 0%\n",
      "Missing URLs: ['https://webmall-3.informatik.uni-mannheim.de/product/asus-dual-rtx4070-super-oc-white-pcie4-12gb-ddr6x-hdmi-3-dp-2550mhz-clock-overclocked']\n",
      "Model response: [\"https://webmall-3.informatik.uni-mannheim.de/product/asus-dual-rtx4070-super-oc-white-pcie4-12gb-ddr6x-hdmi-3-dp-2550mhz-clock-overclocked/\"]\n",
      "[\"https://webmall-3.informatik.uni-mannheim.de/product/asus-dual-rtx4070-super-oc-white-pcie4-12gb-ddr6x-hdmi-3-dp-2550mhz-clock-overclocked/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_EndToEnd_Task7\n",
      "Task: \\nFind the cheapest offer for the Asus ROG Ryuo III 360 ARGB 360mm Liquid CPU Cooler and the cheapes...\n",
      "MCP Recall: 100% | Model Accuracy: 50%\n",
      "Missing URLs: ['https://webmall-3.informatik.uni-mannheim.de/product/asus-rog-ryuo-iii-360-argb-360mm-liquid-cpu-cooler-8th-gen-asetek-pump-3-x-argb-fans-anime-matrix-led-display-white-edition']\n",
      "Model response: [\"https://webmall-1.informatik.uni-mannheim.de/product/asus-rog-ryuo-iii-360-argb-360mm-liquid-cpu-cooler-8th-gen-asetek-pump-3-x-argb-fans-anime-matrix-led-display-white-edition/\",\"https://webmall-3.informatik.uni-mannheim.de/product/corsair-vengeance-lpx-16gb-kit-2-x-8gb-ddr4-3000mhz-pc4-24000-cl16-xmp-2-0-dimm-memory/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_EndToEnd_Task8\n",
      "Task: \\nFind the cheapest offer for the Be Quiet! Pure Base 500 Gaming Case and the cheapest offer for a P...\n",
      "MCP Recall: 100% | Model Accuracy: 50%\n",
      "Missing URLs: ['https://webmall-2.informatik.uni-mannheim.de/product/fractal-design-560w-ion-2-psu-fully-modular-fluid-dynamic-fan-80-platinum-ultraflex-wires-whisper-quiet-mode']\n",
      "Model response: [\"https://webmall-2.informatik.uni-mannheim.de/product/be-quiet-pure-base-500-gaming-case-with-window-atx-2-x-pure-wings-2-fans-psu-shroud-black/\",\"https://webmall-2.informatik.uni-mannheim.de/product/gigabyte-p550b-power-supply-unit-550-w-204-pin-atx-atx-black/\"]\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== CASE 2: Model found additional (incorrect) solutions ===\n",
      "Total cases: 52\n",
      "\n",
      "Detailed analysis:\n",
      "\n",
      "Task ID: Webmall_Single_Product_Search_Task3\n",
      "Task: \\nFind all offers for the Samsung Galaxy A25 and A35.\\n...\n",
      "Accuracy: 75% | Precision: 75%\n",
      "Additional URLs: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Single_Product_Search_Task4\n",
      "Task: \\nFind all offers for the Asus ROG Ryujin II ARGB 360mm Liquid CPU Cooler.\\n...\n",
      "Accuracy: 100% | Precision: 67%\n",
      "Additional URLs: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "Task ID: Webmall_Single_Product_Search_Task6\n",
      "Task: \\nFind all offers for a Corsair Vengeance 32GB Kit (2 x 16GB) DDR5.\\n...\n",
      "Accuracy: 64% | Precision: 88%\n",
      "Additional URLs: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "=== CASE 3: MCP didn't retrieve all correct solutions ===\n",
      "Total cases: 29\n",
      "\n",
      "Breakdown by category:\n",
      "                                   mcp_recall        metrics\n",
      "                                        count  mean <lambda>\n",
      "category                                                    \n",
      "Webmall_Best_Fit_Specific                   4  0.55     0.55\n",
      "Webmall_Best_Fit_Vague                      2  0.33     0.33\n",
      "Webmall_Cheapest_Best_Fit_Specific          4  0.00     0.00\n",
      "Webmall_Cheapest_Best_Fit_Vague             4  0.27     0.06\n",
      "Webmall_Checkout                            8  0.00     0.00\n",
      "Webmall_EndToEnd                            1  0.00     0.00\n",
      "Webmall_Find_Compatible_Products            3  0.00     0.00\n",
      "Webmall_Substitutes                         3  0.22     0.00\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis Function - Adapts to different data formats\n",
    "def analyze_errors(df):\n",
    "    \"\"\"\n",
    "    Analyze errors in different evaluation approaches.\n",
    "    Handles both RAG-based and MCP-based evaluation formats.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check available columns to determine data format\n",
    "    has_rag_coverage = 'rag_coverage' in df.columns\n",
    "    has_mcp_metrics = 'mcp_metrics' in df.columns\n",
    "    has_nested_metrics = 'metrics' in df.columns and isinstance(df['metrics'].iloc[0], dict)\n",
    "    has_separate_metrics = 'recall' in df.columns and 'precision' in df.columns\n",
    "    \n",
    "    print(\"=== ERROR ANALYSIS ===\")\n",
    "    print(f\"Data format detected: \", end=\"\")\n",
    "    \n",
    "    if has_rag_coverage:\n",
    "        print(\"RAG-based evaluation\")\n",
    "        return analyze_rag_errors(df)\n",
    "    elif has_mcp_metrics:\n",
    "        print(\"MCP-based evaluation\")\n",
    "        return analyze_mcp_errors(df)\n",
    "    elif has_nested_metrics or has_separate_metrics:\n",
    "        print(\"Standard metrics evaluation\")\n",
    "        return analyze_standard_errors(df)\n",
    "    else:\n",
    "        print(\"Unknown format - attempting basic analysis\")\n",
    "        return analyze_basic_errors(df)\n",
    "\n",
    "def analyze_rag_errors(df):\n",
    "    \"\"\"Analyze errors in RAG-based evaluation format\"\"\"\n",
    "    print(\"\\n=== CASE 1: RAG found all solutions but model missed some ===\")\n",
    "    \n",
    "    # Filter for cases where RAG found everything but model didn't\n",
    "    case1 = df[(df['rag_coverage'] == 1.0) & (df['accuracy'] < 1.0)]\n",
    "    print(f\"Total cases: {len(case1)}\")\n",
    "    \n",
    "    if len(case1) > 0:\n",
    "        print(\"\\nDetailed analysis:\")\n",
    "        for idx, row in case1.iterrows():  # Show first 3 for brevity\n",
    "            print(f\"\\nTask ID: {row['task_id']}\")\n",
    "            print(f\"Task: {row.get('task', row.get('user_task', 'N/A'))[:100]}...\")\n",
    "            print(f\"RAG Coverage: {row['rag_coverage']:.0%} | Model Accuracy: {row['accuracy']:.0%}\")\n",
    "            print(f\"Missing URLs: {row['missing_urls']}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n=== CASE 2: Model found additional (incorrect) solutions ===\")\n",
    "    \n",
    "    # Filter for cases where model returned extra URLs\n",
    "    case2 = df[df['additional_urls'].apply(lambda x: len(x) > 0)]\n",
    "    print(f\"Total cases: {len(case2)}\")\n",
    "    \n",
    "    if len(case2) > 0:\n",
    "        print(\"\\nDetailed analysis:\")\n",
    "        for idx, row in case2.head(3).iterrows():\n",
    "            print(f\"\\nTask ID: {row['task_id']}\")\n",
    "            print(f\"Task: {row.get('task', row.get('user_task', 'N/A'))[:100]}...\")\n",
    "            print(f\"Accuracy: {row['accuracy']:.0%} | Precision: {row['precision']:.0%}\")\n",
    "            print(f\"Additional URLs: {len(row['additional_urls'])}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n=== CASE 3: RAG didn't retrieve all correct solutions ===\")\n",
    "    \n",
    "    # Filter for cases where RAG missed some correct answers\n",
    "    case3 = df[df['rag_coverage'] < 1.0]\n",
    "    print(f\"Total cases: {len(case3)}\")\n",
    "    \n",
    "    if len(case3) > 0:\n",
    "        print(\"\\nBreakdown by category:\")\n",
    "        category_stats = case3.groupby('category').agg({\n",
    "            'rag_coverage': ['count', 'mean'],\n",
    "            'accuracy': 'mean'\n",
    "        }).round(2)\n",
    "        print(category_stats)\n",
    "    \n",
    "    return case1, case2, case3\n",
    "\n",
    "def analyze_mcp_errors(df):\n",
    "    \"\"\"Analyze errors in MCP-based evaluation format\"\"\"\n",
    "    print(\"\\n=== CASE 1: MCP found all solutions but model missed some ===\")\n",
    "    \n",
    "    # Calculate MCP efficiency (recall from MCP retrieval)\n",
    "    df['mcp_recall'] = df['mcp_metrics'].apply(lambda x: x.get('recall', 0))\n",
    "    df['mcp_precision'] = df['mcp_metrics'].apply(lambda x: x.get('precision', 0))\n",
    "    \n",
    "    # Filter for cases where MCP found everything but model didn't\n",
    "    case1 = df[(df['mcp_recall'] == 1.0) & (df['metrics'].apply(lambda x: x.get('accuracy', 0)) < 1.0)]\n",
    "    print(f\"Total cases: {len(case1)}\")\n",
    "    \n",
    "    if len(case1) > 0:\n",
    "        print(\"\\nDetailed analysis:\")\n",
    "        for idx, row in case1.iterrows():\n",
    "            print(f\"\\nTask ID: {row['task_id']}\")\n",
    "            print(f\"Task: {row.get('task', 'N/A')[:100]}...\")\n",
    "            print(f\"MCP Recall: {row['mcp_recall']:.0%} | Model Accuracy: {row['metrics']['accuracy']:.0%}\")\n",
    "            print(f\"Missing URLs: {row['missing_urls']}\")\n",
    "            print(f\"Model response: {row['raw_response']}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n=== CASE 2: Model found additional (incorrect) solutions ===\")\n",
    "    \n",
    "    # Filter for cases where model returned extra URLs\n",
    "    case2 = df[df['additional_urls'].apply(lambda x: len(x) > 0)]\n",
    "    print(f\"Total cases: {len(case2)}\")\n",
    "    \n",
    "    if len(case2) > 0:\n",
    "        print(\"\\nDetailed analysis:\")\n",
    "        for idx, row in case2.head(3).iterrows():\n",
    "            print(f\"\\nTask ID: {row['task_id']}\")\n",
    "            print(f\"Task: {row.get('task', 'N/A')[:100]}...\")\n",
    "            print(f\"Accuracy: {row['metrics']['accuracy']:.0%} | Precision: {row['metrics']['precision']:.0%}\")\n",
    "            print(f\"Additional URLs: {len(row['additional_urls'])}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n=== CASE 3: MCP didn't retrieve all correct solutions ===\")\n",
    "    \n",
    "    # Filter for cases where MCP missed some correct answers\n",
    "    case3 = df[df['mcp_recall'] < 1.0]\n",
    "    print(f\"Total cases: {len(case3)}\")\n",
    "    \n",
    "    if len(case3) > 0:\n",
    "        print(\"\\nBreakdown by category:\")\n",
    "        category_stats = case3.groupby('category').agg({\n",
    "            'mcp_recall': ['count', 'mean'],\n",
    "            'metrics': lambda x: x.apply(lambda m: m.get('accuracy', 0)).mean()\n",
    "        }).round(2)\n",
    "        print(category_stats)\n",
    "    \n",
    "    return case1, case2, case3\n",
    "\n",
    "def analyze_standard_errors(df):\n",
    "    \"\"\"Analyze errors in standard metrics format\"\"\"\n",
    "    print(\"\\n=== CASE 1: Perfect recall but imperfect accuracy ===\")\n",
    "    \n",
    "    # Get recall values (nested or separate)\n",
    "    if 'recall' in df.columns:\n",
    "        recall_values = df['recall']\n",
    "        accuracy_values = df['accuracy']  \n",
    "        precision_values = df['precision']\n",
    "    else:\n",
    "        recall_values = df['metrics'].apply(lambda x: x.get('recall', 0))\n",
    "        accuracy_values = df['metrics'].apply(lambda x: x.get('accuracy', 0))\n",
    "        precision_values = df['metrics'].apply(lambda x: x.get('precision', 0))\n",
    "    \n",
    "    # Filter for cases where recall is perfect but accuracy isn't\n",
    "    case1 = df[(recall_values == 1.0) & (accuracy_values < 1.0)]\n",
    "    print(f\"Total cases: {len(case1)}\")\n",
    "    \n",
    "    if len(case1) > 0:\n",
    "        print(\"\\nDetailed analysis:\")\n",
    "        for idx, row in case1.head(3).iterrows():\n",
    "            recall = recall_values.iloc[idx] if hasattr(recall_values, 'iloc') else recall_values[idx]\n",
    "            accuracy = accuracy_values.iloc[idx] if hasattr(accuracy_values, 'iloc') else accuracy_values[idx]\n",
    "            print(f\"\\nTask ID: {row['task_id']}\")\n",
    "            print(f\"Task: {row.get('task', 'N/A')[:100]}...\")\n",
    "            print(f\"Recall: {recall:.0%} | Accuracy: {accuracy:.0%}\")\n",
    "            print(f\"Missing URLs: {row.get('missing_urls', 'N/A')}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n=== CASE 2: Model found additional (incorrect) solutions ===\")\n",
    "    \n",
    "    # Filter for cases where model returned extra URLs\n",
    "    case2 = df[df['additional_urls'].apply(lambda x: len(x) > 0)]\n",
    "    print(f\"Total cases: {len(case2)}\")\n",
    "    \n",
    "    if len(case2) > 0:\n",
    "        print(\"\\nDetailed analysis:\")\n",
    "        for idx, row in case2.head(3).iterrows():\n",
    "            accuracy = accuracy_values.iloc[idx] if hasattr(accuracy_values, 'iloc') else accuracy_values[idx]\n",
    "            precision = precision_values.iloc[idx] if hasattr(precision_values, 'iloc') else precision_values[idx]\n",
    "            print(f\"\\nTask ID: {row['task_id']}\")\n",
    "            print(f\"Task: {row.get('task', 'N/A')[:100]}...\")\n",
    "            print(f\"Accuracy: {accuracy:.0%} | Precision: {precision:.0%}\")\n",
    "            print(f\"Additional URLs: {len(row['additional_urls'])}\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\n=== CASE 3: Imperfect recall ===\")\n",
    "    \n",
    "    # Filter for cases where recall is imperfect\n",
    "    case3 = df[recall_values < 1.0]\n",
    "    print(f\"Total cases: {len(case3)}\")\n",
    "    \n",
    "    if len(case3) > 0:\n",
    "        print(\"\\nBreakdown by category:\")\n",
    "        category_stats = case3.groupby('category').agg({\n",
    "            'task_id': 'count',\n",
    "            'recall': 'mean' if 'recall' in df.columns else lambda x: x.apply(lambda r: r.get('recall', 0) if isinstance(r, dict) else 0).mean(),\n",
    "            'accuracy': 'mean' if 'accuracy' in df.columns else lambda x: x.apply(lambda r: r.get('accuracy', 0) if isinstance(r, dict) else 0).mean()\n",
    "        }).round(2)\n",
    "        print(category_stats)\n",
    "    \n",
    "    return case1, case2, case3\n",
    "\n",
    "def analyze_basic_errors(df):\n",
    "    \"\"\"Basic error analysis for unknown formats\"\"\"\n",
    "    print(\"\\nPerforming basic error analysis...\")\n",
    "    \n",
    "    # Try to find any available metrics\n",
    "    available_cols = df.columns.tolist()\n",
    "    print(f\"Available columns: {available_cols}\")\n",
    "    \n",
    "    if 'additional_urls' in df.columns:\n",
    "        case2 = df[df['additional_urls'].apply(lambda x: len(x) > 0)]\n",
    "        print(f\"Tasks with additional URLs: {len(case2)}\")\n",
    "    \n",
    "    if 'missing_urls' in df.columns:\n",
    "        case3 = df[df['missing_urls'].apply(lambda x: len(x) > 0)]\n",
    "        print(f\"Tasks with missing URLs: {len(case3)}\")\n",
    "    \n",
    "    return None, None, None\n",
    "\n",
    "# Run the analysis\n",
    "case1, case2, case3 = analyze_errors(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ERROR ANALYSIS SUMMARY ===\n",
      "Total tasks analyzed: 91\n",
      "\n",
      "Error breakdown:\n",
      "Case 1 (Perfect retrieval, imperfect model): 15 (16.5%)\n",
      "Case 2 (Model added incorrect URLs): 52 (57.1%)\n",
      "Case 3 (Imperfect retrieval): 29 (31.9%)\n",
      "\n",
      "Error overlaps:\n",
      "Tasks with both Case 1 & 2 errors: 14\n",
      "Tasks with both Case 2 & 3 errors: 20\n",
      "\n",
      "Perfect performance (100% accuracy & precision): 29 (31.9%)\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics for error analysis\n",
    "def print_error_summary(df, case1, case2, case3):\n",
    "    \"\"\"Print summary statistics for error analysis\"\"\"\n",
    "    print(\"\\n=== ERROR ANALYSIS SUMMARY ===\")\n",
    "    \n",
    "    total_tasks = len(df)\n",
    "    \n",
    "    # Handle case where cases might be None (for basic analysis)\n",
    "    case1_count = len(case1) if case1 is not None else 0\n",
    "    case2_count = len(case2) if case2 is not None else 0\n",
    "    case3_count = len(case3) if case3 is not None else 0\n",
    "    \n",
    "    print(f\"Total tasks analyzed: {total_tasks}\")\n",
    "    print(f\"\\nError breakdown:\")\n",
    "    print(f\"Case 1 (Perfect retrieval, imperfect model): {case1_count} ({case1_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Case 2 (Model added incorrect URLs): {case2_count} ({case2_count/total_tasks*100:.1f}%)\")\n",
    "    print(f\"Case 3 (Imperfect retrieval): {case3_count} ({case3_count/total_tasks*100:.1f}%)\")\n",
    "    \n",
    "    # Calculate overlaps if possible\n",
    "    if case1 is not None and case2 is not None:\n",
    "        case1_and_2 = len(set(case1.index).intersection(set(case2.index)))\n",
    "        print(f\"\\nError overlaps:\")\n",
    "        print(f\"Tasks with both Case 1 & 2 errors: {case1_and_2}\")\n",
    "        \n",
    "        if case3 is not None:\n",
    "            case2_and_3 = len(set(case2.index).intersection(set(case3.index)))\n",
    "            print(f\"Tasks with both Case 2 & 3 errors: {case2_and_3}\")\n",
    "    \n",
    "    # Perfect performance analysis\n",
    "    try:\n",
    "        # Try different ways to access accuracy and precision\n",
    "        if 'accuracy' in df.columns and 'precision' in df.columns:\n",
    "            perfect = len(df[(df['accuracy'] == 1.0) & (df['precision'] == 1.0)])\n",
    "        elif 'metrics' in df.columns:\n",
    "            perfect = len(df[\n",
    "                (df['metrics'].apply(lambda x: x.get('accuracy', 0)) == 1.0) & \n",
    "                (df['metrics'].apply(lambda x: x.get('precision', 0)) == 1.0)\n",
    "            ])\n",
    "        else:\n",
    "            perfect = 0\n",
    "        \n",
    "        print(f\"\\nPerfect performance (100% accuracy & precision): {perfect} ({perfect/total_tasks*100:.1f}%)\")\n",
    "    except:\n",
    "        print(\"\\nPerfect performance calculation not available for this data format\")\n",
    "\n",
    "# Print the summary\n",
    "print_error_summary(df, case1, case2, case3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Analysis: MCP vs Model Performance Comparison\n",
    "def analyze_mcp_vs_model_performance(df):\n",
    "    \"\"\"\n",
    "    Compare MCP retrieval performance vs model performance\n",
    "    Only available for datasets with MCP metrics\n",
    "    \"\"\"\n",
    "    if 'mcp_metrics' not in df.columns:\n",
    "        print(\"MCP metrics not available in this dataset\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n=== MCP VS MODEL PERFORMANCE COMPARISON ===\")\n",
    "    \n",
    "    # Extract metrics\n",
    "    mcp_precision = df['mcp_metrics'].apply(lambda x: x.get('precision', 0))\n",
    "    mcp_recall = df['mcp_metrics'].apply(lambda x: x.get('recall', 0))\n",
    "    model_precision = df['metrics'].apply(lambda x: x.get('precision', 0))\n",
    "    model_recall = df['metrics'].apply(lambda x: x.get('recall', 0))\n",
    "    \n",
    "    # Overall comparison\n",
    "    print(f\"MCP Retrieval vs Model Performance:\")\n",
    "    print(f\"  MCP Precision: {mcp_precision.mean():.3f} | Model Precision: {model_precision.mean():.3f}\")\n",
    "    print(f\"  MCP Recall: {mcp_recall.mean():.3f} | Model Recall: {model_recall.mean():.3f}\")\n",
    "    \n",
    "    # Category-wise comparison\n",
    "    print(f\"\\nCategory-wise comparison:\")\n",
    "    category_comparison = df.groupby('category').agg({\n",
    "        'mcp_metrics': lambda x: {\n",
    "            'precision': x.apply(lambda m: m.get('precision', 0)).mean(),\n",
    "            'recall': x.apply(lambda m: m.get('recall', 0)).mean()\n",
    "        },\n",
    "        'metrics': lambda x: {\n",
    "            'precision': x.apply(lambda m: m.get('precision', 0)).mean(),\n",
    "            'recall': x.apply(lambda m: m.get('recall', 0)).mean()\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    for idx, row in category_comparison.iterrows():\n",
    "        print(f\"\\n{idx}:\")\n",
    "        print(f\"  MCP - Precision: {row['mcp_metrics']['precision']:.3f}, Recall: {row['mcp_metrics']['recall']:.3f}\")\n",
    "        print(f\"  Model - Precision: {row['metrics']['precision']:.3f}, Recall: {row['metrics']['recall']:.3f}\")\n",
    "    \n",
    "    # Cases where MCP performed better than model\n",
    "    mcp_better_precision = df[mcp_precision > model_precision]\n",
    "    mcp_better_recall = df[mcp_recall > model_recall]\n",
    "    \n",
    "    print(f\"\\nMCP advantages:\")\n",
    "    print(f\"  Tasks where MCP had better precision: {len(mcp_better_precision)} ({len(mcp_better_precision)/len(df)*100:.1f}%)\")\n",
    "    print(f\"  Tasks where MCP had better recall: {len(mcp_better_recall)} ({len(mcp_better_recall)/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Run MCP vs Model analysis if applicable\n",
    "analyze_mcp_vs_model_performance(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CATEGORY OVERVIEW ===\n",
      "Available categories:\n",
      "  Webmall_Add_To_Cart: 7/7 tasks complete (100.0%)\n",
      "  Webmall_Best_Fit_Specific: 5/11 tasks complete (45.5%)\n",
      "  Webmall_Best_Fit_Vague: 1/8 tasks complete (12.5%)\n",
      "  Webmall_Cheapest_Best_Fit_Specific: 1/10 tasks complete (10.0%)\n",
      "  Webmall_Cheapest_Best_Fit_Vague: 0/6 tasks complete (0.0%)\n",
      "  Webmall_Cheapest_Product_Search: 7/10 tasks complete (70.0%)\n",
      "  Webmall_Checkout: 0/8 tasks complete (0.0%)\n",
      "  Webmall_EndToEnd: 6/8 tasks complete (75.0%)\n",
      "  Webmall_Find_Compatible_Products: 0/5 tasks complete (0.0%)\n",
      "  Webmall_Single_Product_Search: 10/12 tasks complete (83.3%)\n",
      "  Webmall_Substitutes: 2/6 tasks complete (33.3%)\n"
     ]
    }
   ],
   "source": [
    "def analyze_incomplete_tasks(df, category_name):\n",
    "    \"\"\"\n",
    "    Analyze all incomplete tasks (completion score != 1) for a given category.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing task results\n",
    "        category_name: Name of the category to analyze\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with detailed analysis of incomplete tasks\n",
    "    \"\"\"\n",
    "    # Filter for the specific category\n",
    "    category_df = df[df['category'] == category_name].copy()\n",
    "    \n",
    "    if len(category_df) == 0:\n",
    "        print(f\"No tasks found for category: {category_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate task completion for each task\n",
    "    task_results = []\n",
    "    \n",
    "    for idx, row in category_df.iterrows():\n",
    "        # Calculate individual task completion\n",
    "        benchmark_set = set(row['correct_answers'])\n",
    "        model_set = set(row['parser_model_answers'])\n",
    "        \n",
    "        task_completion = 1 if benchmark_set == model_set else 0\n",
    "        \n",
    "        # Calculate precision and recall for this task\n",
    "        if len(model_set) > 0:\n",
    "            precision = len(benchmark_set.intersection(model_set)) / len(model_set)\n",
    "        else:\n",
    "            precision = 0.0\n",
    "            \n",
    "        if len(benchmark_set) > 0:\n",
    "            recall = len(benchmark_set.intersection(model_set)) / len(benchmark_set)\n",
    "        else:\n",
    "            recall = 0.0\n",
    "        \n",
    "        # Calculate differences\n",
    "        missing_urls = benchmark_set - model_set\n",
    "        additional_urls = model_set - benchmark_set\n",
    "        \n",
    "        task_results.append({\n",
    "            'task_id': row['task_id'],\n",
    "            'task_description': row['user_task'],\n",
    "            'task_completion': task_completion,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'benchmark_answers': sorted(list(benchmark_set)),\n",
    "            'model_answers': sorted(list(model_set)),\n",
    "            'missing_urls': sorted(list(missing_urls)),\n",
    "            'additional_urls': sorted(list(additional_urls)),\n",
    "            'benchmark_count': len(benchmark_set),\n",
    "            'model_count': len(model_set)\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(task_results)\n",
    "    \n",
    "    # Filter for incomplete tasks only\n",
    "    incomplete_tasks = results_df[results_df['task_completion'] != 1]\n",
    "    \n",
    "    print(f\"=== INCOMPLETE TASKS ANALYSIS FOR {category_name} ===\")\n",
    "    print(f\"Total tasks in category: {len(results_df)}\")\n",
    "    print(f\"Incomplete tasks: {len(incomplete_tasks)}\")\n",
    "    print(f\"Completion rate: {(len(results_df) - len(incomplete_tasks)) / len(results_df) * 100:.1f}%\")\n",
    "    \n",
    "    if len(incomplete_tasks) == 0:\n",
    "        print(\"All tasks in this category are complete!\")\n",
    "        return results_df\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Display each incomplete task\n",
    "    for idx, task in incomplete_tasks.iterrows():\n",
    "        print(f\"\\n📋 TASK: {task['task_id']}\")\n",
    "        print(f\"Description: {task['task_description'].strip()}\")\n",
    "        print(f\"Completion: {task['task_completion']} | Precision: {task['precision']:.3f} | Recall: {task['recall']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n✅ BENCHMARK ANSWERS ({task['benchmark_count']} URLs):\")\n",
    "        for i, url in enumerate(task['benchmark_answers'], 1):\n",
    "            print(f\"  {i}. {url}\")\n",
    "        \n",
    "        print(f\"\\n🤖 MODEL ANSWERS ({task['model_count']} URLs):\")\n",
    "        for i, url in enumerate(task['model_answers'], 1):\n",
    "            print(f\"  {i}. {url}\")\n",
    "        \n",
    "        if task['missing_urls']:\n",
    "            print(f\"\\n❌ MISSING URLs ({len(task['missing_urls'])}):\")\n",
    "            for i, url in enumerate(task['missing_urls'], 1):\n",
    "                print(f\"  {i}. {url}\")\n",
    "        \n",
    "        if task['additional_urls']:\n",
    "            print(f\"\\n➕ ADDITIONAL URLs ({len(task['additional_urls'])}):\")\n",
    "            for i, url in enumerate(task['additional_urls'], 1):\n",
    "                print(f\"  {i}. {url}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "    \n",
    "    return incomplete_tasks\n",
    "\n",
    "# Example usage function\n",
    "def show_category_options(df):\n",
    "    \"\"\"Show available categories and their completion rates\"\"\"\n",
    "    print(\"Available categories:\")\n",
    "    category_stats = df.groupby('category').agg({\n",
    "        'task_id': 'count',\n",
    "        'correct_answers': lambda x: sum(1 for benchmark, model in zip(x, df.loc[x.index, 'parser_model_answers']) \n",
    "                                        if set(benchmark) == set(model)),\n",
    "    }).reset_index()\n",
    "    \n",
    "    category_stats['completion_rate'] = category_stats['correct_answers'] / category_stats['task_id']\n",
    "    category_stats = category_stats.rename(columns={'task_id': 'total_tasks', 'correct_answers': 'completed_tasks'})\n",
    "    \n",
    "    for _, row in category_stats.iterrows():\n",
    "        print(f\"  {row['category']}: {row['completed_tasks']}/{row['total_tasks']} tasks complete ({row['completion_rate']:.1%})\")\n",
    "    \n",
    "    return category_stats\n",
    "\n",
    "# Show available categories\n",
    "print(\"=== CATEGORY OVERVIEW ===\")\n",
    "category_overview = show_category_options(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== INCOMPLETE TASKS ANALYSIS FOR Webmall_Substitutes ===\n",
      "Total tasks in category: 6\n",
      "Incomplete tasks: 4\n",
      "Completion rate: 33.3%\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📋 TASK: Webmall_Substitutes_Task1\n",
      "Description: \\nFind the cheapest alternative for this item: https://webmall-3.informatik.uni-mannheim.de/product/arctic-liquid-freezer-iii-360mm-liquid-cpu-cooler-p12-pwm-pst-fans-pwm-controlled-pump .\\n\n",
      "Completion: 0 | Precision: 0.000 | Recall: 0.000\n",
      "\n",
      "✅ BENCHMARK ANSWERS (1 URLs):\n",
      "  1. https://webmall-2.informatik.uni-mannheim.de/product/arctic-liquid-freezer-iii-360mm-liquid-cpu-cooler-p12-pwm-pst-fans-amp-pwm-controlled-pump\n",
      "\n",
      "🤖 MODEL ANSWERS (1 URLs):\n",
      "  1. https://webmall-2.informatik.uni-mannheim.de/product/arctic-liquid-freezer-iii-360mm-liquid-cpu-cooler-p12-pwm-pst-fans-pwm-controlled-pump\n",
      "\n",
      "❌ MISSING URLs (1):\n",
      "  1. https://webmall-2.informatik.uni-mannheim.de/product/arctic-liquid-freezer-iii-360mm-liquid-cpu-cooler-p12-pwm-pst-fans-amp-pwm-controlled-pump\n",
      "\n",
      "➕ ADDITIONAL URLs (1):\n",
      "  1. https://webmall-2.informatik.uni-mannheim.de/product/arctic-liquid-freezer-iii-360mm-liquid-cpu-cooler-p12-pwm-pst-fans-pwm-controlled-pump\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📋 TASK: Webmall_Substitutes_Task3\n",
      "Description: \\nFind cheaper alternatives with at least the same capacity and speed for this product: https://webmall-3.informatik.uni-mannheim.de/product/corsair-1tb-mp600-core-xt-m-2-nvme-ssd-m-2-2280-pcie4-3d-qlc-nand-r-w-5000-3500-mb-s-700k-900k-iops . If multiple offers share the lowest price, return all of them.\\n\n",
      "Completion: 0 | Precision: 0.333 | Recall: 1.000\n",
      "\n",
      "✅ BENCHMARK ANSWERS (1 URLs):\n",
      "  1. https://webmall-4.informatik.uni-mannheim.de/product/team-1tb-mp44-m-2-nvme-gen4-ssd-m-2-2280-pcie4-r-w-7400-6500-mb-s-heat-dissipating-graphene-label\n",
      "\n",
      "🤖 MODEL ANSWERS (3 URLs):\n",
      "  1. https://webmall-3.informatik.uni-mannheim.de/product/kingston-1tb-nv2-m-2-nvme-ssd-m-2-2280-pcie4-r-w-3500-2100-mb-s\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/team-1tb-mp44-m-2-nvme-gen4-ssd-m-2-2280-pcie4-r-w-7400-6500-mb-s-heat-dissipating-graphene-label\n",
      "  3. https://webmall-4.informatik.uni-mannheim.de/product/team-1tb-mp44-m-2-nvme-gen4-ssd-m-2-2280-pcie4-r-w-7400-6500-mb-s-heat-dissipating-graphene-label\n",
      "\n",
      "➕ ADDITIONAL URLs (2):\n",
      "  1. https://webmall-3.informatik.uni-mannheim.de/product/kingston-1tb-nv2-m-2-nvme-ssd-m-2-2280-pcie4-r-w-3500-2100-mb-s\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/team-1tb-mp44-m-2-nvme-gen4-ssd-m-2-2280-pcie4-r-w-7400-6500-mb-s-heat-dissipating-graphene-label\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📋 TASK: Webmall_Substitutes_Task4\n",
      "Description: \\nFind cheaper alternatives for this monitor having the same size: https://webmall-1.informatik.uni-mannheim.de/product/asus-27-tuf-gaming-monitor-vg279q3a-1920-x-1080-fast-ips-1ms-elmb-180hz-variable-overdrive-99-srgb-vesa . If multiple offers share the lowest price, return all of them.\\n\n",
      "Completion: 0 | Precision: 0.000 | Recall: 0.000\n",
      "\n",
      "✅ BENCHMARK ANSWERS (2 URLs):\n",
      "  1. https://webmall-1.informatik.uni-mannheim.de/product/asus-27-frameless-eye-care-gaming-monitor-va27dqfr-ips-1920-x-1080-1ms-100hz-vga-hdmi-dp-adaptive-sync-vesa\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/asus-27-frameless-eye-care-monitor-va27dqsb-ips-1920-x-1080-75hz-adaptive-sync-usb-hub-vesa\n",
      "\n",
      "🤖 MODEL ANSWERS (2 URLs):\n",
      "  1. https://webmall-3.informatik.uni-mannheim.de/product/asus-vp279-27-inch-full-hd-led-backlight-lcd-monitor-hdmi-display-port-vga\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/asus-vp279-27-inch-full-hd-led-backlight-lcd-monitor-hdmi-display-port-vga-2\n",
      "\n",
      "❌ MISSING URLs (2):\n",
      "  1. https://webmall-1.informatik.uni-mannheim.de/product/asus-27-frameless-eye-care-gaming-monitor-va27dqfr-ips-1920-x-1080-1ms-100hz-vga-hdmi-dp-adaptive-sync-vesa\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/asus-27-frameless-eye-care-monitor-va27dqsb-ips-1920-x-1080-75hz-adaptive-sync-usb-hub-vesa\n",
      "\n",
      "➕ ADDITIONAL URLs (2):\n",
      "  1. https://webmall-3.informatik.uni-mannheim.de/product/asus-vp279-27-inch-full-hd-led-backlight-lcd-monitor-hdmi-display-port-vga\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/asus-vp279-27-inch-full-hd-led-backlight-lcd-monitor-hdmi-display-port-vga-2\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "📋 TASK: Webmall_Substitutes_Task6\n",
      "Description: \\nFind cheaper alternatives to this keyboard that have a gaming aesthetic: https://webmall-1.informatik.uni-mannheim.de/product/asus-rog-strix-scope-ii-nx-snow-mechanical-rgb-gaming-keyboard-rog-nx-snow-linear-switches-sound-dampening-pbt-keycaps-intuitive-controls . If multiple offers share the lowest price, return all of them.\\n\n",
      "Completion: 0 | Precision: 0.333 | Recall: 0.333\n",
      "\n",
      "✅ BENCHMARK ANSWERS (3 URLs):\n",
      "  1. https://webmall-1.informatik.uni-mannheim.de/product/asus-tuf-gaming-k1-rgb-keyboard-with-volume-knob-19-key-rollover-side-light-bar-armoury-crate-spill-resistant-detachable-wrist-rest\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/asus-rog-strix-scope-ii-nx-snow-mechanical-rgb-gaming-keyboard-rog-nx-snow-linear-switches-sound-dampening-pbt-keycaps-intuitive-controls\n",
      "  3. https://webmall-3.informatik.uni-mannheim.de/product/asus-tuf-gaming-k1-rgb-keyboard-with-volume-knob-19-key-rollover-side-light-bar-armoury-crate-spill-resistant-detachable-wrist-rest\n",
      "\n",
      "🤖 MODEL ANSWERS (3 URLs):\n",
      "  1. https://webmall-3.informatik.uni-mannheim.de/product/asus-rog-strix-scope-ii-nx-snow-mechanical-rgb-gaming-keyboard-rog-nx-snow-linear-switches-sound-dampening-pbt-keycaps-intuitive-controls\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/gamemax-pulse-rgb-usb-gaming-keyboard-mouse-kit\n",
      "  3. https://webmall-3.informatik.uni-mannheim.de/product/pictek-pc244-black\n",
      "\n",
      "❌ MISSING URLs (2):\n",
      "  1. https://webmall-1.informatik.uni-mannheim.de/product/asus-tuf-gaming-k1-rgb-keyboard-with-volume-knob-19-key-rollover-side-light-bar-armoury-crate-spill-resistant-detachable-wrist-rest\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/asus-tuf-gaming-k1-rgb-keyboard-with-volume-knob-19-key-rollover-side-light-bar-armoury-crate-spill-resistant-detachable-wrist-rest\n",
      "\n",
      "➕ ADDITIONAL URLs (2):\n",
      "  1. https://webmall-3.informatik.uni-mannheim.de/product/gamemax-pulse-rgb-usb-gaming-keyboard-mouse-kit\n",
      "  2. https://webmall-3.informatik.uni-mannheim.de/product/pictek-pc244-black\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>task_description</th>\n",
       "      <th>task_completion</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>benchmark_answers</th>\n",
       "      <th>model_answers</th>\n",
       "      <th>missing_urls</th>\n",
       "      <th>additional_urls</th>\n",
       "      <th>benchmark_count</th>\n",
       "      <th>model_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webmall_Substitutes_Task1</td>\n",
       "      <td>\\nFind the cheapest alternative for this item:...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[https://webmall-2.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-2.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-2.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-2.informatik.uni-mannheim.de/...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Webmall_Substitutes_Task3</td>\n",
       "      <td>\\nFind cheaper alternatives with at least the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>[https://webmall-4.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Webmall_Substitutes_Task4</td>\n",
       "      <td>\\nFind cheaper alternatives for this monitor h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Webmall_Substitutes_Task6</td>\n",
       "      <td>\\nFind cheaper alternatives to this keyboard t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-1.informatik.uni-mannheim.de/...</td>\n",
       "      <td>[https://webmall-3.informatik.uni-mannheim.de/...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     task_id  \\\n",
       "0  Webmall_Substitutes_Task1   \n",
       "2  Webmall_Substitutes_Task3   \n",
       "3  Webmall_Substitutes_Task4   \n",
       "5  Webmall_Substitutes_Task6   \n",
       "\n",
       "                                    task_description  task_completion  \\\n",
       "0  \\nFind the cheapest alternative for this item:...                0   \n",
       "2  \\nFind cheaper alternatives with at least the ...                0   \n",
       "3  \\nFind cheaper alternatives for this monitor h...                0   \n",
       "5  \\nFind cheaper alternatives to this keyboard t...                0   \n",
       "\n",
       "   precision    recall                                  benchmark_answers  \\\n",
       "0   0.000000  0.000000  [https://webmall-2.informatik.uni-mannheim.de/...   \n",
       "2   0.333333  1.000000  [https://webmall-4.informatik.uni-mannheim.de/...   \n",
       "3   0.000000  0.000000  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "5   0.333333  0.333333  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "\n",
       "                                       model_answers  \\\n",
       "0  [https://webmall-2.informatik.uni-mannheim.de/...   \n",
       "2  [https://webmall-3.informatik.uni-mannheim.de/...   \n",
       "3  [https://webmall-3.informatik.uni-mannheim.de/...   \n",
       "5  [https://webmall-3.informatik.uni-mannheim.de/...   \n",
       "\n",
       "                                        missing_urls  \\\n",
       "0  [https://webmall-2.informatik.uni-mannheim.de/...   \n",
       "2                                                 []   \n",
       "3  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "5  [https://webmall-1.informatik.uni-mannheim.de/...   \n",
       "\n",
       "                                     additional_urls  benchmark_count  \\\n",
       "0  [https://webmall-2.informatik.uni-mannheim.de/...                1   \n",
       "2  [https://webmall-3.informatik.uni-mannheim.de/...                1   \n",
       "3  [https://webmall-3.informatik.uni-mannheim.de/...                2   \n",
       "5  [https://webmall-3.informatik.uni-mannheim.de/...                3   \n",
       "\n",
       "   model_count  \n",
       "0            1  \n",
       "2            3  \n",
       "3            2  \n",
       "5            3  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze_incomplete_tasks(df, \"Webmall_Substitutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - analyze incomplete tasks for a specific category\n",
    "# Uncomment and run the line below to analyze a specific category:\n",
    "# analyze_incomplete_tasks(df, \"Webmall_Best_Fit_Specific\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
